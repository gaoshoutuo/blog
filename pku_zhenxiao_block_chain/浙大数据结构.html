
<html lang="en">
    <head>

        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=0.5">

        <link href="http://www.yinwang.org/main.css" rel="stylesheet" type="text/css">
        <link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed">
        <link rel="shortcut icon" href="http://www.yinwang.org/images/Yc.jpg">

        <title>自律</title>

        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                                    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-45586344-1', 'yinwang.org');
            ga('send', 'pageview');
        </script>
    </head>

    <body>
        <script>
            if (/mobile/i.test(navigator.userAgent) || /android/i.test(navigator.userAgent))
            {
               document.body.classList.add('mobile');
            }
        </script>

        <!--
        <div class="ad-banner" style="margin-bottom: 5px">
            <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
            <ins class="adsbygoogle"
                    style="display:inline-block;width:100%;height:90px"
                    data-ad-client="ca-pub-1331524016319584"
                    data-ad-slot="6657867155"></ins>
            <script>(adsbygoogle = window.adsbygoogle || []).push({});</script>
        </div>
        -->

        <div class="inner">
            <h2>陈越姥姥的数据结构 PAT</h2>
            <p>兜兜转转 蹉跎好多年，就是花不出时间片来学习，来来回回好几次。要深入理解，需要借助PAT的一些实现题目来巩固自己的学习成果。这次一定行，加油。别贪心一点一点来，
            每天晚上时间利用起来虚空刷leetcode题目，公务员题目也不能放弃</p>
            <p>1.基本概念
                <span>
                    又听到姥姥的声音果然很怀念，第一章内容我都差不多急着。最长子列和的N3 N2 Nlogn N四个，就分治不算特别的明白。递归跟迭代的内存区别都已经明白了。
                    一对一线性结构
                    一对多 树结构
                    多对多就是图 集合了。
                </span>
            </p>
            <p>2.线性结构
                <span>
                  老何熟悉的课程。主要介绍的要点有
                    1.多项式的三种表示方式，及其抽象数据结构流程以及实现。
                    2.线性表的扩展 广义表，双重链表，多种链表，term
                    3.堆栈的抽象数据结构及其实现以及后缀表达式 前缀表达式，
                    11111-----多项式的三种表达方式 1 就是数组，数组下标表示其项，2类型数组结构数组也就是能够解决稀疏多项式的问题，3链表结构，也能解决稀疏多项式的问题。
                    对于数组，他的线性结构应该有定义节点类型也就是数据对象集，定义其操作集。一般有6种，make empty，findK位，insert，delete，find是否有node，length。对于数组而言插入删除的要整体的
                    前后移动效率不高，而且非排列findNode的查找效率也是On，那么链表对应的操作集也就这些，但是要对其定义一个自己的数据对象集，链表插入删除也方便，但是查找也不方便。其插入如下 先找一个P节点指向
                    i位置，s节点为新节点，s->next=p->next,p->next=s，顺序不能乱。删除操作要先由p节点i节点的前一位，后由q节点指向i节点。p-next=q-next,再对q节点回收数据。
                    22222-----广义双重链表这里很很大的空间，不要仅仅视野局限单一编程语言与单链表。下次一定？
                    33333-----前缀中缀后缀这些东西其实由树来的，但是一下子给你中缀求前缀后缀表达式都不太好求，由于有括号的存在lisp的前缀似乎可以比较的线性。但是后缀表达式在堆栈当中直接被利用。12.2我就看到这里先。

                    12.3 的确很有必要重新看一遍，我发现慢慢学，less is more 这种原则就很好，很舒服。内容很多 while p1&&p2就是两个条件至少一个被破坏才跳出
                    1.堆栈 抽象数据结构 数据集 就是堆栈stack>anthingType<，的这样一个东西。其操作集合 还是老5样 创建为空 堆栈长度 是否满 push加入 pop弹出栈顶，值得一提的是，它是先进后出的结构，想想羽毛球的球筒结构就知道。其中包含一个top的标志字段来指示当前的栈顶位置，其初始值为-1，
                    其数组的实现是   push: stack.data[++top]=item;  pop:return stack.data[top--] 其之精髓之意在这里，不考虑超出长度的情况先。中间引入如何用单个数组存储两个堆栈呢，也简单从两边向中间挤，top2-top1==1则为数组满。这样也不浪费，值得注意的是由于是两个堆栈，所以
                    又得加一个额外的标志位tag来标识加入到哪一个堆栈当中。当然我们还有链表的实现方式，先建模，我们是在堆栈顶部也就是链表的头部做操作，也就像是345这样的堆栈链表，加入头部加入2345，删除也是头部删除45，
                    有一个S节点始终指向栈顶节点，也就是数组中的top之意思。PUSH：create firsecell; first.emement=item; firstcell.next=s.next;s.next=firstcell.其pop: firstcell=s.next;s.next=firstcell.next; free firstcell;
                    堆栈的使用场景有 程序递归的调用堆栈，深度优先算法DFS，回溯backtrace算法多项式的后缀表达式，等，如果没有这个结构递归都不知道怎么去做。将中缀表达式算出需要做两步，先通过堆栈自己定义算法来变成缀表达式，再通过堆栈算一次。
                    2.队列，也是一种有所限制的线性结构。其表现形式为FIFO先来先服务，就像我们日常生活中的排队一样，第二点就是队列首部删除队列尾部插入。其抽象数据结构的数据集合，Queue的泛型数组，对了有道算法题是跟泛型数组与对象数组有什么区别的。
                    其实现方式也是两种数组跟链表，数组上的实现，队首出列之后就有空间剩余，这样会造成浪费，于是就加入了新机制环数组，但是这样满了与零状态又没法分辨，所以要们就不存满，要么就多一个标志位看看是否满。
                    好了我们就单一数组来研究其实现方法，它有两个变量 front表示对手需要删除的节点初始为-1 rear为尾部节点，初始值也为-1，然后都是递增的。push：queue.data[++rear]=element;  pop:return queue.data[++front];而链表的方式 front链表头，用来删除，rear是链表尾用来新增。
                    具体模型  front为头节点与rear为尾节点， push: q.rearNode.next=itemNode;rearNode=itemNode; pop deNode=q.frontNode.next; q,frontNode,next=deNode.next;free(deNode);大概是这样。后面就是多项式相加的小白专场了其实也没多大意思，大概就是如何使用链表来做，做点是算法如何去组织。

                </span>
            </p>

            <p>3.树 上
                <span>
                  这次看这个视频最大的目的就是为了来看树的，因为我在leetcode上面的刷题已经到了树那里了，但是对一些树的知识并不深刻，所以这次回来看一下数据结构，当然也是回应musk对于创业者想说的那句话，把时间专注在那些能令你受益终身的事情上，大概这个意思回头在看看。
                    那么什么是树呢，树就是一种用来表示层次结构最高效率的结构，一般有两种形式，静态与动态，我们的查字典就是最常见的静态查找方式，而设计数据库文件系统，文件目录的人要考虑那种动态的树结构，因为数据伴随这增删改查。等等。
                    但是说回来树最多的操作就是，插入，删除，以及遍历。修改涉及到遍历超查找这里也归为一类。。一般我们顺序结构都不是从0位置开始存储的，而是从1位开始存储，而从最后面--的循环开始查找，0位放哨兵，用来警示边界。
                    这种顺序查找的时间复杂度是On，我们选择二分查找，因为它是Ologn 但是二分查找一般适合在数组或者一些顺序的结构当中去做。如果我们把二分查找用一颗树来表示的话，是一样的。
                    那么树的深度一般的[logn]+1 ,ASL树的平均查找次数是(4*4+4*3+2*2+1)/n =3 这样。此外我们需要引入二叉查找树，因为这不仅仅是查找方便，节点的插入与删除也方便。有一些树的基本性质需要认知，树节点除根节点外有且仅有一个父节点，N个节点就有N条边，
                    术语:1.节点度 degree 就是树的分叉个数，树有几颗子树，2.树的度也就是树中节点最大的度，3.叶节点就代表度为0的树节点没有子树，4.兄弟节点也就是拥有同一个父节点的树节点。5.父节点，子节点不用在介绍了。6.
                    路径及路径长度，就是两个节点之间经过的最小边的个数，不清楚能够来一个父子回旋找表亲。7.祖先 孙子节点都是指这些的一类集合，肖老师那里的MT proof似乎有所提到，8节点层次，从1开始，根节点为1，9。树深也就是树的最大层次。
                    对于一个任意树而言有结构链表来保存节点，但是这样要么就是链表节点的结构不统一，要么就是浪费空间。所以我们引入一个新的表示方法，儿子兄弟表示法，即建立一个树节点，有left data right，左节点用来保存子节点指针，右节点用来表示最近的兄弟节点，没有的话用null表示，
                    DATA可以表示任意类型的数据。通过这种表示出来的树，再倾斜45度角，我们惊奇的发现了，这居然就是一种度为2的树，我们清晰的统一了树的结构，二叉树可以用来表示任意树。这可就派上大用场了，也就是说我们可以通过研究二叉树树性质，去解决
                    所有数据的问题。对于二叉树而言，存在这一些特殊的二叉树，斜二叉树，完美二叉树，完全二叉树，完全包含完美。对于其实现我们有两种方式，要么数组存储，要么链表存储，数据在存储完全完美二叉树上面拥有奇效。
                    对于存储，我们不仅仅要是考虑存，更重要的是如何能够读去与查找相应的节点，以及找到他们的子节点父节点。数组存储对于完全二叉树而言，找父节点则是[i/2],左子就是2i,右子就是2i+1，对于二叉树又有一种特殊的性质，n0=n2+1;.它的证明方法也不难，
                    n0+n1+n2-1=0*n0+n1+2*n2,也就是一棵树从上看从下看的不同边的边的个数相等。一般情况下我们都是用链表来实现二叉树的。这样既直观又万能好吧。好了我们说说以链表实现的树的遍历方式，总共有4种前序，中序，后序，层次。他们分别如下。
                    前序遍历:中->前->后
                        preOrderTraverSal(tree){
                            if(tree!=null)
                            printf(tree.data)
                            preOrderTraverSal(tree.left)
                            preOrderTraverSal(tree.right)
                    }
                    中序遍历:前->中->后
                      midOrderTraverSal(tree){
                            if(tree!=null)
                            midOrderTraverSal(tree.left)
                              printf(tree.data)
                            midOrderTraverSal(tree.right)
                    }
                    后序遍历:前->后->中
                     afterOrderTraverSal(tree){
                            if(tree!=null)
                            afterOrderTraverSal(tree.left)
                            afterOrderTraverSal(tree.right)
                              printf(tree.data)
                    }
                    但是不管任何的遍历方式总是On的 而且对于遍历而言用的空间也是On的，这里这张顺序图令我想到钱几个月学SICP的时候，斐波那契数列的那种非常差的不利用中间状态的递归图。
                    也许肖老师说的对，函数式编程语言熟练之后，读起来会非常的简明易用。
                    当然二叉树也可以有堆栈的遍历非递归式样的遍历，这就是leetcode上面一开始我感觉疑惑没学会儿后回来学的东西。以中序遍历为例子来表示非递归遍历我们的二叉树。
                    非递归式的迭代遍历:主要思想以中序遍历为例，就是遇到左节点先压入栈，处理中间节点，然后把右节点赋给树节点，再做循环。用代码做演示。
                    BT bt; Stack stack;
                    while(stack!=null||bt!=null){
                    while(bt!=null){
                        stack.push(bt);
                             printf(bt)//写在这里的逻辑是输出前序
                        bt=bt.left;
                    }
                    if(stack!=null){
                        bt=stack.pop()
                    printf(bt)//写在这里的逻辑是输出中序
                        bt=bt.right;
                    //后序写在这里吗 no
                    }
                    }
                    但是后序的写法可没那么简单
                      while(stack!=null||bt!=null){
                    while(bt!=null){
                        stack.push(bt);
                             printf(bt)//写在这里的逻辑是输出前序
                        bt=bt.left;
                    }
                    //前面的格式相同但是后面就不同了 要对节点新增一个标志字段 用来表示有没有被作为右节点来过
                    bt=pop();
                    stack.push(bt);//push回去
                    bt=bt.right
                    if(bt==null||isT==true)//空或者来过 出栈
                    {printf 出栈}
                    else if(bt！=null&&isT！=true)//入栈
                    {设置true 并且入栈}

                    我估摸着是这个意思。
                    。拉尼娜是西水高东水低，厄尔尼诺是东水高西水低。
                    层次遍历:我还以为层序遍历没什么花头呢，结果层序遍历的确是有点东西嗷。
                    首先它是通过队列实现的，也就是按照如下的工作顺序。1把根节点保存入队列，把根节点的左右节点保存入节点，在从队列中读取一个节点，重复做这个操作，知道队列为空为止。
                    此外我们还可以通过不同的对树节点的遍历顺序，去做很多事情例如。
                    1.寻找叶子节点，即可以在任何以递归定义的前中后序的遍历方式增加一个if判断方式，即判断左右节点是否为空来判断是否叶子节点再做一定的操作。我发现浙大数据结构的这种结构与思维都很简单，简洁有力的思想与程序。
                    2.求树的高度。当然也好做，不过要后序遍历，因为思想是递归计算左右子树的树高的max+1，以算树高。其程序遍历到底部是return 0;其他是return max+1.的这种程序的做法。这个程序值得去写一个
                    3.表达式表示树的先序 中序 后序的遍历会生成不同的前缀中缀后缀表达式，但是中缀的运算优先级有问题，需要给中序添加括号来修正。
                    4.从两个序列来构造树，关键在于必须其中一个为中序遍历序列，否则无法形成唯一树。做法很简单，首先以先中序遍历为例，我们从先序遍历的头就是根节点，然后去找中序序列中的根节点位置，前半部分就是左子树，后半部分就是右子树，然后回到前序序列，
                    分出左子树，右子树长度等等，然后左子树中的头右是左树的根节点，再重复做，也要递归左弄完右。
                    小白专场的 树通过问题 t1通过左右交换可以变化为t2则认为两树同构。这里我们使用结构链表来做比较方便输入的数据集是如下，后面指示左右节点在结构数组中的哪里。
                   4
                    A -1 1
                    B 2 3
                    C -1 -1
                    D -1 -1
                    这样，当然也可以乱序的，我们寻找根节点可以找没有指向的节点。
                    4
                    B 4 3
                    A - 0

                    D -1 -1
                    C -1 -1
                    这样的话0234都是有指向的所以只有1是没有指向则为根节点。感觉程序还得自己去写一遍，我也在某种意义上面的小白虽然听懂过了一遍，但是没有写过程序印象不深刻，还得深刻深刻下去。
                    魔鬼处于必须深刻的把知识印到脑子里与行动之上。
                </span>
            </p>
            <p>4.树 中
                <span>硅谷第六季 你知道我因为真的在乎道德理想准则，这一路上使得我吃了多少苦吗。谈论道德，talk is cheap  Understanding Compression [Book]看硅谷突然对压缩的思路 历史 算法感兴趣，此外我感兴趣交易选股算法。以此来执行。
                    美剧硅谷最后deAI能够来破解密码学的大质数。its feature not a bug.多项式时间的离散对数通用解。we build a monster we need to kill it。知道能成功就会有人无限去尝试，逆向。必须悲惨的弄死自己这没有跟随者。
                    1.二叉搜索树
                    定义就是树结构，的左节点小于自己，右节点大于自己，并且左右子树也满足这个规律。把数放在树上，线性的查找变成树上二分查找，并且树的操作非常灵活。内容并不是很多，讲课结构依旧是经典的抽象数据结构模式，讲它的操作集及其实现。
                    操作集包含 find findMax findMin insert delete等 我们一一来为其实现。
                        1.find 可递归也可循环，因为是尾递归，思想就是跟当前节点比较，如果大于当前则去右边找，如果小于当前则去左边找，否则想到就是找到返回，while也可以方便的实现。
                        2.findMax则是 一直往右边递归或者迭代直到NULL
                        3.findMin则是 一直往左边递归或者迭代直到NULL
                        4.Insert 跟find类似 逐级的去摸排大小，然后插在某个地方，递归跟迭代都可以
                        5.delete 删除就麻烦一点了，思想就是要分清删除的节点是什么节点，对于叶子节点我们知道可以直接删除，对于单一子节点的节点，也可以直接删除，并且把自己的子节点按照原来的left right顺序原原本本的插入，就像做了一个链表插入删除手术一样。
                    但是对于双子节点的删除，就有两种方面把被删除的左子树的最大节点拿来替换，并且被替换节点A的所有子树都插在A的父节点也就是子树的祖父节点上。或者拿要删节点
                    的右子树的最小节点替换，它的左右子树都并到祖父节点上，代码复杂一点但并不难，耐心总可以写出来。
                    美剧规矩的技术道德，是在是令人敬佩啊。
                    2.二叉平衡树 B树 b+树  重复学换个老师学就通了
                    申明：本文高度，深度基数为1，但是在《数据结构与算法分析:java语言描述》这本书上，高度，深度的基数为0；两种记法都没有错，都可以用来描述树的性质，只需要标注（>0）或者(>=0)做一个区分和解释即可
                    节点n的高度 : n节点到叶子节点所有路径上包含节点个数的最大值。叶子节点的高度为1，往上节点的高度依次递增。
                    节点n的深度 : 从根节点到节点n唯一的路径的长，根节点深度为1
                    前面我们看到了，不同的插入顺序生产的不同的树，的树高度以及AVL都不一样。平均查找次数。树高度从0开始，从根结点出发最大边数，树深度就从1开始，这两句话不清楚嗷。所以我们希望树的高度尽量不要太高，于是就有了平衡二叉树，它也是搜索树。
                    平衡二叉树的定义是，任意节点的左子树以及右子树的高度之差绝对值 <=1 ,对任意节点而言。我们也很容易证明平衡二叉树的高度H与节点个数N之间存在Olog2n 的关系。对于平衡二叉树，我们有 Nh-1+Nh-2 +1=Nh,这就有点类似斐波那契数列了，我们把递推公式改为通项公式才能去验证Olog2n
                    不得已斐波那契数列也从0开始
                    H    Nh    Fh
                    0   1      1
                    1   2       1
                    2   4       2
                    3   7       3
                    4   12      5
                    5   20      8
                    6   33      13
                    7   54      21
                    8   88      34
                    Nh=Fh+2 -1  Fh 通项又是([5]^(1/2)+1)^h/[5]^(1/2)  差不多可以推出N与H的关系就是H=Olog2N的关系。
                    那么平衡二叉树 作为一种抽象数据结构，查找方便，也是有抽象数据集以及操作集的，无外乎就是老三样 find findMax findMin insert delete这些。我们来给它一一的去实现。
                    但是这里并不是去实现，但是实现也是很好实现的，只不过。这边注重的是调整。而且要多一个平衡因子，遍历这个平衡树的时候前面状态应该也要保存，不然也容易找不到的吧。find那些没什么花头都是一样的，我们重点看insert与delete，因为平衡的存在，所以插入或者删除会导致不平衡的出现。这个时候
                    就需要对树进行调整，总共有4种调整方式他们都包含了所有的不平衡需要调整的情况，这里面的知识溯源讲的不到位，但是倒是把知识点讲透了，大学陶老师就更差了感觉很赶任务一样，都不认真不仔细环境。
                    1.LL 也就是新节点插入到被破坏节点的左边的左边上，无论插在左与右边，这样顺着的ABC三个节点，就要把B提上来，B右节点的BR要给C当左节点。B的右节点是A。B的左节点是C。
                    2.RR 也就是新节点插入到被破坏节点的左边的左边上，无论插在左与右边，这样顺着的ABC三个节点，就要把B提上来，B左节点BL要给A当右节点。B的右节点是C。B的左节点是A。
                    3.LR 旋转 也就是新节点插入到被破坏节点的左边的右边上，无论插在那个节点的左与右边，这样原先的ABC就变成，C左边为B，右边为A的形式，此外CL也就是C的左节点要被当作B的右节点，CR也就是C的右节点要被当作A的左节点。因为二叉搜索树，A>CR>C>CL>B
                    4.RL 选择 也就是新节点插入到被破坏节点的左边的右边上，无论插在那个节点的左与右边，这样原先的ABC就变成,C左边为A，右边为B的形式,此外CL也就是C的左节点要被当作A的右节点，CR也就是C的右节点要被当作B的左节点。因为二叉搜索树，B>CR>C>CL>A
                    这四者关系在数据结构中的运用。
                    具体可以看笔记 也可以看ProcessOn 去怎么把笔记跟电脑笔记做漂亮，一次完美就是一件很重要的事情。
                    小白专场 不同序列生成的二叉搜索树去验证是否相同，当然是先生成然后遍历一遍看看序列是否相同。
                    1.建立树 然后看左右树是否相同
                    2.不建立树 第一个节点是根节点，大小分出左右子树，然后再划分，最后比较序列是否相同。
                    3.建立一颗基准树，其他都与此树相比较。树节点要多一个flag标志是否有被访问过
                    就是被测试序列的插入在基准树上，假如途中遇到没见过的节点就是不一致，如果途中节点都flag为0一致，那么就是一致的。这个程序需要对树中的flag进行reset，与没用到的就free。
                </span>
            </p>

            <p>5.树 下
                <span>//别想了 quant一般都是cs，数学，统计的博士搞的，听他说两句就觉得自己能赚钱那简直天方夜谭。情商势利眼女
                    1.堆 这个东西我当时看过最大堆最小堆，感觉一步步推导的过程摸不着头脑。好的我明白了最大堆了，我推一下最小堆，只要条件相反就行了
                    堆思想其实就是优先队列的Ologn化，对比较看重找到或者删除最大最小或者第几大的元素，堆并不是搜索树，搜索树最大最小在左右两边，找下去得花OlogN时间，我们要求的是有限要O1，根节点就是极大极小值，对于其抽象数据类型，
                    抽象数据集，就是树节点的根要大于左右两边任意一个子树最大，或者小于任意子树最小。其次要求他是一个完全二叉树，因为实现上，堆可以通过有数组链表或者有序数组链表，但是我们在C中还是以数组来实现堆的存储，也就是堆的生成
                    要指定一个固定长度，当然这个固定长度指的大多数时候都不是堆包含的节点个数，只是说最大能包含多少。create(len)大概是这样，然后我们来说说其操作集合，最重要的也就是insert delete create等等。
                    1.create 上面说过了 我们这里的insert与delete也以数组为准 On
                    2.先说delete因为比较容易解决，delete一般都是delete根节点，似乎不支持删除其他的任意节点。为了保持其完全二叉树的性质。好像所谓的删除与堆栈的pop相同要return出根节点，这个数据结构不是为了存储那么多正常数据，而是一些优先级数据的自调整，
                    delete掉根节点之后要把最末尾的完全叶节点顶替上来，然后进行一轮从上到下的维持最大堆的行为。具体删除要花费LogN个时间。
                    3.insert，有两种实现，一种就是插入一个节点修改成一次最大堆，再插入一个再修改成一次最大堆。这种实现的ONlogN，并不算太快的算法。我们还有更快的另一种算法，也就是先按照序列顺序，先生成完全二叉树，
                    然后我们从第N/2个节点开始做。一直到根节点去维护最大或者最小堆的这种属性，然后再从最上往下做一次最大最小堆结点的调整，最后是On个数量级的复杂度，因为他的调整次数就相当于节点的高度总和，节点的高度也就是到叶节点的边的个数，叶节点的高度为0。
                    最大堆有意思他，优先队列当中的节点优先级是在实时的变化的。堆中平衡二叉树同构，对同一个父节点的叶子节点的左右是可以交换的。   这里还有一点 就是头顶上这个节点 要跟随着达到最大最小堆根节点为止

                    2.huffman 哈夫曼树 与 哈夫曼编码 想起数据压缩入门 Oreilly ，数据压缩算法，硅谷的数据压缩也是突破极限，但是极限在于数学，信息论。诶上世纪的控制论信息论牛啊。数据压缩原理与应用(第二版)，数据压缩入门  这尼玛头上在搞shama装修.
                    huffman树解决的是不同出现频率的数据应该如何去建立二叉树存储，以使得它能够以比较小的数据量去存储，其某种意义上限应该是NLogN，但是这是相当于等长信息熵的，我们的哈夫曼树编码是不等长的。这仍然可以带来编码的节约。
                    但是要避免编码的二义性质，最好是前缀码，意思就在于每个编码都位于叶节点，这样就不存在歧义，但是这种变长编码的读取，是不每次都要去编码树上去做，每次找到叶节点就算做一个。
                    此外哈夫曼编码具备一些性质
                    1不存在度为1的节点
                    2节点总个数为2倍叶节点-1
                    3左右子树交换依然是哈夫曼编码
                    4同一串编码依然存在同构
                    WPL最小也就是huffman树，在huffman编码中有应用，其建立过程就是从最小的两个频率节点开始，组装成一个新的节点，再跟一个最小的组合成一个新节点，新节点的左右节点为刚刚两个最小节点，将新节点重新插入最小堆，再如此。其中用到了最小堆，可以提
                    供最小节点。
                    硅谷的魔笛手公司pieper，引发我对于数据压缩做法以及算法的兴趣，其次哈夫曼编码最好其实也就是信息熵大小的高度。但是为了可读性也会牺牲性能。哈夫曼树为了解决，已知概率而设计最好的ASL方案的一颗树
                    3.集合以及集合运算 集合运算 加减交并查，这里面着重介绍 交集与查两个操作
                    我们通过反向的，从子节点指向父节点的树形式，也称为双亲表示法，很奇怪的名字也看不出为啥，总之这不是二叉树，连二叉也不是，何老师使用结构数组去存储这样的集合，应该是集合123都存在一个结构数组中。
                    结构数组的使用方法，专门有一个positive来存储其父节点所在数组的index
                    查操作分2步，
                    1.循环查到这个元素是否存在该结构数据当中，如果不在直接返回-1，表示不在结构数组当中
                    2.查到该元素，返回该positive不是-1的话，就返回index继续查，直到查到-1，返回这个根节点所在的index。这就算查
                    并操作就是把一个的根节点positive改成另一个根节点。陈越姥姥的那个题目意思就是查操作第一步碰到单向链表那样的数据，可能就是On2 而且不需要结构数组，普通数组也可以，直接就把一些数据映射到0-N-1的这个范围上。
                    。。。意思就是那些数据要映射在0-N-1上，所以我说计算机就是抽象与映射。

                </span>
            </p>

            <p>6.图 上 哇靠终于是姥姥来讲课了  培养兴趣还是快乐 先骗进来杀 我上次感觉姥姥讲课不如老何啊
                <span>
                    1.图介绍 感觉姥姥的课有点贵了嗷。
                    最短路径 陈家庄到王村最短 以及最小生成树的问题 村村通怎么最短。
                    图的多对多包含线性以及树的关系。DFS BFS shortPath最短路 MST最小生成树。
                    图的抽象数据结构。
                    数据集 图包含顶点以及边，图不能没有顶点可以没有边，边可以是双向的也可以是单向的，但是不能为环或者重向，两个节点之间只能有一条边。
                    操作集：insertV，insertS,deleteS,deleteV,DFS,BFS,shortPath,MST.
                    常见术语:无向图，有向图，权重图-网络。以后的东西用到在说。
                    如何在程序中表示一个图，两种方法。
                    邻接矩阵法:G[N][N].问题网络中的Vi Vj，就是开一个二维数组去存储，很明显对角线上全为0，因为没有自回路，并且是对称的，有浪费，实际情况可以用一个G[N*(N+1)/2]的一维数组，的确是要多出N/2的个数。
                    那么这么存储方式也带来了新的问题，也就是一个原本的G[i][j]的值在这上面就不好找了。我们就有新的公式，i*(i+1)/2+j,就是原先G[i][j]再此的坐标。也很明白就是上面那个三角形按照原来上面的公式多出i/2,
                    后面那行的j就是多余的偏移量加上去。
                    对于邻接矩阵表示法，优点在于:1.表示非常的简洁易懂，方便检查任意一对节点是否有边，方便找任意节点所有的邻节点即行列非0，方便计算任意节点的度，有向图的出度与入度，行非零为出度，列非零为入度，无向图就是行列的非0元素。
                    缺点在于:浪费空间，浪费时间
                    假如没有连接的话需要用什么来表示也是0吗。
                    第二种表示法:邻接表 G[N] 指针数组，每个节点连接什么一次用链表挂上去，每个节点也没被存储两次，也不怎么省空间，一定要比较稀疏才可以用。因为用到了N个节点+2E个节点，
                    有点在于方便找到一个节点所有的邻节点，对于无向图而言是方便计算度，但是对于有向图也只是出度方面，入度不方便，入度需要建立逆邻接表，即所有原先邻接矩阵列的数据，并且
                    不方便检查任意两个节点之间是否存在边，能检查不方便。
                    实际上图并不仅仅有这两种方法去表示，就是之前的数组与链表应该也是这样的。

                    2.图遍历
                    图遍历解决一些有趣的问题
                    DFS 深度优先搜索 类似树遍历中的先序遍历
                    对于邻接表就是O(V+E),每个每个边都访问一次，对于邻接矩阵就是O(N2)有N行，每行都访问一边。
                    void DFS(V)
                        visit[v]=true
                        for(V的邻接点W)
                            w为false DFS[w]

                    BFS 广度优先搜索 姥姥用的是迭代循环 我先入为主的用递归。树的层序遍历，也要要到队列 要记住 奥赛 竞赛 学习书
                    void BFS(V)
                        visit[v]=true
                        addQueue(Q)
                    while(Q not empty)
                        v=delQ;
                        for(V周围的W)
                            if visit[W]==false
                                visit[w]=true
                                addQueue(W)
                    图中的其他概念
                    连通:图中任意两节点都有路径
                    路径:图中所有顶点连接之集合
                    连通图:所有节点都连通的图
                    回路:起点与终点同一个的图
                    连通分量:要么不能新增节点导致非联通图，要不不能额外新增边
                    强连通:无向的连通图

                    设计有趣味的，设计有意思的。趣味性
                    我们为什么要设计两种不同的遍历方式呢，结果是不一样的，若是我们遍历那些不连通的图怎么办呢，就需要借助这两个遍历方式
                    for(G 中的V)
                        DFS(V)/BFS(V)
                    3.应用实例-1 暴露年龄的007 图编码
                    007跳跃距离，鳄鱼邻接表，到岸边的距离，然后DFS 找到就是找到 找不到就是找不到。数学题目一般都是抽象出来，计算机也讲抽象，但是没数学这么抽象三生万物的感觉，计算机有这些有趣的实例题目来做。

                    4.应用实例-2    社交6度空间理论，就是V节点邻接结构表，这个结构数组要增加一个字段来显示layer。算法思路就是进行广度优先搜索，这样可以看层数，进行深优先行不行，应该可以。
                    然后广度层序遍历在到自己，layer++,这样
                    5.小白专场 建立图 很简单邻接矩阵  WeightType G[MaxNum][MaxNum] int vNum;int eNum; =1为由边 =0为没有 这样对于一个砌猪圈的程序员是已经够了，但是对于我们这些高大上的工程师可是不够的，
                    我们工程师考虑的问题，或许是在一个大的系统中去设计一个小的模块，满足两点性质，高可用性通用性，以及可读性，能够被其他同事看懂。建立图要先建立结构，然后一条条的为图建立边，
                    虽然香港比较的珠光宝气，拜金封建，可是
                </span>
            </p>

            <p>7.图 中
                <span>
                    1.3个树习题，但是为什么要在图里面再讲。上传资料总是不改变还是12年的，感觉就有一点沽名钓誉。算了 DFS BFS
                     这应该是树的习题，各位对于知识是如何去对待的呢，就像你们在学校中学到，正视你们学习态度。
                    树的再遍历 堆栈非递归的中序遍历  对于push就是先序，对于pop就是中序。然后我们的先序中序出来了，后序也能出来了。问题最开始的序列要建立树，或者建立堆栈。
                    2 完全二叉树
                    3 huffman code 一定是最优的，但是最优不一定huffman。判断最优编码一定要建立huffman树，因为满足没有度为1，以及是前缀码不一定就是最优WPL。

                    2.最短路:最短路包含的问题其实是一系列的问题，并不仅仅是最短路，知识图论当中的最短路，其他还包含最少价格，最少停靠之类，这些都是有其他字段在帮忙平时要多多留意
                    单源:从固定源点出发去到求其到所有其他顶点最短路径
                        无权图：其实就是类似BFS的算法，只要有边就+1，但是我们这里不需要visit[v],我们需要新建两个数组，dist[w]=dist[v]+1,path[w]=v;意思就是后一个的距离就是前一个节点的距离+1.当然还得通过距离不可能是某个奇特的值，最后我们通过
                    最后的w节点可以一路反推回，但是要没一个都可以推出前一个节点v，直到推到起点，但是顺序列还是要依靠我们的堆栈来做逆向。
                    时间复杂度O E+V 但是这是邻接表的情况下，我们还有邻接矩阵的情况
                    这个条件。path[w]保存到其的前一个节点，这样
                        单源有权图:最短路，这是有权了要加权了，仍然有负值圈这点要比较，要记住我们是在递增的寻找最短路。Dijkstra,金光闪闪登场，比较复杂，思想就是S集合包含s源点以及一系列的vi节点，
                    min(dist[w],dist[v]+v->w),这里的意思就是新增节点v使得原先的dist[w]变小，那么v一定过程性在这个最短路当中，因为最短并且不存在其他vi会在 s-v-w ,v的前部分，因为递增的关系，这已经是最笑了，
                    s-vi-v-w一定比s-v-w长，而且也不可能在后面，v到w仅仅是直接的一条边，因为这是递增的dist[v],这种时候此时dist[vi]=dist[v]+vvi 一定大于dist[v],也就不是最短路。倘若这个vi存在那它早就递增被收入S当中成为V了，不会仍然在外面当W。
                    V根据贪心是集合中距离最长的。人保持聪明与洞察，穿越人群的时候要小心翼翼的保护自己。
                    重新开一个，因为有很多需要改变的地方。dijkstra算法的精要意义，
                    1在于S{s源点以及那些确定了最短路径的Vi}，
                    2 dist[v]代表v到源点的最短路径，要保证每次dist[v]比较最短，该路径必须经过S中的顶点，中间节点必须是属于S中的其他Vi，这样虽然不一定是最短的，但每次总是能相对最短。这个是变化当中的最短。
                    3 默认的假设  路径必须是递增的，路径是啥，就是dist[v],贪心每次取最小，路径也就是慢慢变大。真正的最短路是必须只经过S中的节点的 为啥，加入最短路中有一个S外面的w，那么dw一定大于dv，也即是说s到v，一定要先到w。s-w-v,这就有矛盾了
                    dw小于dv，那dw早就应该作为v被加入到S集合中了，V的递增保证了不存在集合外的W使得s-w-v最小，w应该是任意的vi，而不应该是节点外的。而我们从dvi小到大去找则感觉上更容易被找到。

                    4 贪心的从每次没有收录的节点选最小的收录进去，这个机制保证了我们的递增
                    5 增加一个v 有可能会影响到其他w的dist值 这句话包含两个意思，v的加入影响dist[w]这首先能说明v必定在短路径上，这里的影响指的是dist[w]>dist[v]+vw,求最短路，我们的dist[w]要不断的变小。寻找这样的v。
                    还有一点就是，v-w必定是直接相连的，中间没有任何的vi，但是有可能有未收录的其他节点位于中的，但不可能是S中的任意vi在中间，在这个节点v-w是只有一条直线，也就是v的新加入必然影响的是其w的最短路，所以在
                    程序中我们就是去看看那些没被收录的v节点的邻节点的w，他们的min dist[w]有没有被新v弄得变小了。这种最短路跟数学证明推理一样，要到三个点去解决。
                    总结就是 我们通过 贪心->dist[v]递增->最短路之经过在S中的顶点，寻找最小的dw。dist[w]不能初始化为-1，而是要变成一个极大的数。果然前面是伪代码，找最小v，每次都是最小堆。
                    对于稠密图而言  邻接表的情况下，寻找V为On 则最后的时间复杂度为O V2+E，稠密情况下边E为V的平方数量级。假如是最小堆来找V就是OlogN，那么最后就是OVlogV+ELogV,对于稀疏图而言，
                    EV同一个数量级所以最后时间复杂度就是OVlogV
                    void dijkstra()
                    while(1)
                        V 中未被收入的dist 最小 要么贪心要么最小堆
                        if(V)不存在 break
                        collection[v]=true
                        for(V邻W)
                            if(dist[w]>dist[v]+vw)dist[w]=dist[v]+vw  path[w]=v

                    多源:首先得理解什么是单源，什么是多源。
                        可以是对所有顶点做单源最短路，最粗暴就是V遍，但是有别的方法比较快速。
                    但是对于稀疏图而言，前面两个方法 稠密 OV2+E 与 稀疏 OVlogV+ElogV，但这里为什么是直接说 多源是OV3+EV,然后floyd算法OV3，去掉了末尾，但是为啥不提
                    前面更好的OVlogV呢，想不通OV2logV。
                    而且多源最短路为啥不设置有向无向呢。
                    void floyd()
                        //初始化邻接矩阵
                        for(int i=0;i< N ;i++)
                            for(int j=0;j< N;j++)
                                D[i][j]=G[i][j]  其他没有连接的部分要初始化为超级大或者MAX值
                                path[i][j]=-1

                    for(int k=0;k< N;k++)
                        for(int i=0;k< N;i++)
                            for(int j=0;k< N;j++)
                                if(D[i][j]< d[i][k]+d[k][j]) {D[i][j]= d[i][k]+d[k][j] 这个是标记最短路的值
                                    path[i][j]=k
                                 }

                    获得任意节点的最短路有公式
                    今天身体疼痛 可能是衣服穿不暖还是穿太紧的问题了，肠道问题我感觉最主要就是屁放不出来的问题
                </span>
            </p>

            <p>8.图 下
                <span>
                    1.最小生成树 MST minmum spanning tree
                    村村通，建立连通图，最少边或者最少路的问题。他的特性
                    是一颗树 没有回路 有V个顶点 V-1条边
                    是生成树 包含全部顶点 V-1条边在图中
                    边的权重最小 而且最小生成树不唯一，而且最小生成树存在图一定连通，图联通最小生成树一定存在。
                    这之中要用到贪心算法，贪心算法的意思在于一个计算要分多个步骤进行，我们每次都取最小权边，选择本次最好的结果不去考虑全局的好坏。当然这里也没有从小到大选那么简单，还要注意3个约束。
                    树没有回路，必须是V-1条边，只有有图中有的边且边权重最小。
                    prim 算法 叫一颗小树生成一颗大树，类似Dijkstra算法，意思就是从一个节点出发，不停的收纳最小节点，集合所邻的最小节点，然后收纳进来，但是对方w节点必须是没有经过过得，必须被未包含，是在外围，
                    这样做下去，假如不存在没被包含的节点就break,的确跟dijkstra算法很像。那么 我来尝试写一下这个算法，其实这个算法跟公式本身都是要理解性的记忆的。
                    void prim()
                        MST=V 首先随便取一个V mst 不一定要去自己构建一颗树，而是parent[index_v]=-1 就是
                        while(1)
                        v=贪心mst中最小  dist[v]初始化正无穷或者Esv
                        收入进入V就是dist[v]=0
                        if(v不存在)break
                        for(v的周围w)
                            if dist[w]!=0
                                if dist[w]>Evw  dist[w]=Evm parent[w]=v //这是在找最小的过程 最小也记录
                    稠密图的效果比较好 OV2 但是对于稀疏图 我们的Kruskal，思想就是把树林合并成森林，其意思就直接就是贪心算法，就是它把每一颗节点都算作是一棵树，不停的收纳最短而且不存在回路情况下的边，
                    然后不停的做，直到收纳V-1 条边为止
                    伪代码
                    void kruskal(){
                        MST={}收纳边  E{}
                        while(边小于V-1&&E中还有边){
                        最小堆的E中招一条最小的边   //抽象非常良好的板块 恰到好处
                        把边在E中删除
                        if(边在MST中不构成回路)边加入MST  //并查集检查是否同一个集和验证是否回路   并查代码要去写一下 不然真的掌握不牢固
                    }
                    }
                    的确就是这样，突然并查集有点忘记了都
                    OE logE
                    2.拓扑排序  例子就是计算机专业课的拓扑网络AOV图
                    V到W有一条有向边那么V一定在W之前，而且
                    拓扑排序就是这么去建立的过程，每次都输出没有前驱顶点的图，入度为0，主要前面的边删去了而且
                    top排序 原来就是拓扑排序，就尼玛离谱

                    void topSort(){
                        for(< V)
                        图中入度最小的节点 V
                    if(V不存在)return error

                    输出V 或者做一些我们想要做的操作
                    for V的每个零节点W Indegree[w]--

                    }
                    加入上面的寻找V设置的不好算法的复杂度就是OV2，是最小堆的话就是Ologn。这里我的方法不太对，反正就是v的邻接点W indegree要--，不如加一层判定，
                    直接把indegree为0的节点W放置在一个容器里面，方便去弄。
                    新算法其实思路相同。
                    我么选择
                      void topSort(){
                    if degree==0  addQ
                        for(图中每个顶点V)
                        while Q is not empty
                    cnt++
                        v=deQ
                            for V邻接点W
                                if --degree[w]==0
                                addQ

                    if(cnt< V-1)return error

                    }

                    top排序的应用就是关键路径问题，这里的新网络AOE不同于上面的AOV，AOV是注重V顶点，AOE是主动流程边E，AOE一般都被安排比较庞大的项目。
                    就是软考里面的不同模块组的开工时间的对比。整个工期有多长，机动时间节点。关键路径就在于没有机动时间的节点组成的路径。所以写程序去解决这个问题。听说关键路径写起来还
                    挺麻烦的。
                    课后习题，旅游规划问题。最短路 直接就是dijkstra，但是稍微加入一些条件，不仅仅是公路的距离，价格，时段堵车什么的，可能都是。
                </span>
            </p>

            <p>9.排序 上  作孽啊，这个大三上的数据结构课又得他妈再来一遍，这下得要过代码，认真认真一板一眼的去学习。普通青年排序
                <span>
                    void X_sort(element[],int N)

                    1.简单排序-冒泡-插入:交换相邻或者往后挪一位
                    Bubble_sort 最好是ON 最差是ON2 ，冒泡排序就是比较简单的，但是On2，数组中不可接受，链表就能够接受了。其意思就是，
                    每次两两交换，然后第一次交换我们的最大的就在后面ON，然后第二次就是第二大ON-1，这样两个for循环弄下去。
                    插入排序 insert_sort其实也是简单的，想象我们摸扑克牌，插入到数组，，但是当10 J Q K，出现9，那么这4张牌都要往后移动。最好On 最坏是Ologn。
                    意思就是 每次从最后一张牌一个数组的位置开始比起，如果小与数组最后值，数组最后值往后挪一个位置，再接着做，直到找到>=的停下来，稍微比上面冒泡算法好的情况就是
                    交换是做三个步骤的，而往后挪就做一个步骤就好了。是碰巧吗 是一样的吧，因为他们的逆序对都是一样的，交换相邻元素正好可以消去一个逆序对。那么插入排序就是
                    ON+I 加入I小于ON的话就是On了，所以对于基本有序的数组排序，插入排序是一个比较有效的排序。简单并且高效。
                    对于任意的序列平均是有N *N-1 /4个逆序对，而每次交换都只消灭一个逆序对，对于任意的交换相邻元素的算法他的最好的时间复杂度就是OmegaN2，想要提高算法的效率，我们每次交换都要消去比较多的
                    逆序对，我们不能仅仅只是交换相邻的元素，而是要跳着跟比较远的元素去交换。
                    难受住 OXT发财机会没抓住，这个币已经在币安上线了，假如我执行我认为的方案去早点搞这种交换的话，已经翻3倍了，但是我应该会早点卖掉吧，
                    2.希尔排序 shell_sort 利用插入排序的简单，并且可以跟比较远的单位去交换。算法分析在排序中的应用，算法导论或者算法4ed 不仅仅是logN N NlogN。
                    思路就是Dk dk-1 dk-2 dk-3.  5间隔序列排序，3间隔排序，1间隔(就是插入排序)，这样前几次得到比较好的排序序列，而且3间隔排序之后5间隔依旧有效，这样方便去插入排序。
                    但是最坏的情况ON2 西格玛 代表就是真正的N2 没有上下界了，在实际当中是三个for循环，Dk=N/2  Dk-1=Dk/2 这样来设计间隔序数。但是实现几间隔数，这是什么算法。
                    那么有可能是增量序列出问题，这样 8 4 2 1 的增量序列容易遗漏以及不能消除逆序对，所以需要有新的增量序列方式。
                    有两个增量序列。一个是 Hibbard dk=2^k -1 相邻元素互质序列 最坏情况 西伽玛N^3/2次方 这个3/2次方是哪里来的也是很奇怪，不多不少上下界固定，只能说真实的算法复杂度不仅仅只有那些整数次方的。最好情况
                    就是西格玛 N^5/4,这个5/4也是很奇怪哪里来的，当然也仅仅只是猜想，到现在为止也没有人能够证明出，计算理论有意思。
                    另一个著名的学者sedgewick，也就是算法4th的作者，高德纳徒弟？提出的 1 5 19 41 109 按照9*4^i-9*2^i+1 或者4^i-3*2^i+1 这两个序列来生产新的序列。T avg=N^7/6  Tworst=N^4/3 我擦这都怎么弄出来的 只有可能是。如果只是对几万的数量级进行
                    排序，希尔排序就告诉我们，算法可以是非常的简单，但是对算法的复杂度分析却十分的困难。
                    3.堆排序: 在开始堆排序的部分，我们要先讲选择排序，也是跳着去改变。算法的伪代码如下。
                    void select_sort(){
                    for(< N-1)
                        minPosition=findMin(A,i,N-1)
                        swap(A[i],A[minPosition])
                    }执行是执行了N-1次 但是还是要看findMin这个算法的时间复杂度。既然是找最小我们的堆就拍上用场了minHeap。这种就是要去建最小堆，所以容易就是NOlogN，建堆开销比较的多而比较的麻烦，
                    所以这里也就能够去理解排序的NlogN问题了，排序是否最快也是NlogN  一般情况下，比较乱的排序NlogN是否就已经是上界了。这里又提了一嘴最小堆直接通过数组构建是线性的也就是OlogN，就是按照N N/2 这样，然后N-1 N-1/2这样。
                    需要额外开一个ON的空间，并且复制元素需要时间。感觉排序这一大堆算法，都没有最优只用哪一个。
                    对于堆排序而言，前面那个算法可太笨了，接下来介绍一个聪明的算法，诶感觉姥姥没抓住这么课的重点多多少少都是有点罗列，女生学的可能是比较好但是教的话感觉还是一些男老师教的好。
                    此外在堆排序当中，假设根节点是烧饼元素或者数组的首位是哨兵的话，对于i的左右子节点应该是2i+1 2i+2.它是最大堆，a[0]是哨兵节点。每次把最大的元素去跟这个完全二叉树堆的最后一个元素去交换，
                    因为排序的最大总是放在最后面的。

                    void Heap_sort(){//我们的堆是用数组存储的所以这样很方便去去除首个。
                        for(i=N/2;i>=0;i--)percDown(A,i,N)//堆的向下过滤   build heap On  最大堆  重新调整就是A0 根节点的调整，因为他这个时候两边都是最大堆了
                        for(N-1,i>0,i--)swap(a[0],a[i])  percDown(A,0,i)  每次把根最大挑战到最后面 然后排除--
                    } 编译器很神奇 我也不会 就知道他的知识图谱的归属。那么他的时间复杂度就是2NlogN-NloglogN  总之比NlogN 大一些，但是总不必去新开一个多余的内存空间了。
                    内存 缓存 涉及到冯诺依曼机当然也涉及到了计算机体系结构等等。处理器芯片设计架构。
                    这里说的比Nlogn好一点不对吧 loglogN肯定小于LogN的啊，虽然堆排序给出的时间复杂度更好，但是这里sedgewick的希尔排序效果却要好一点。
                    这点我同意 但是很明显 N^7/6 基本小某个值之前基本都小于Nlogn 我是这么理解的。
                    4.归并排序
                    归并排序的核心意思就是两个有序子序列的归并。假设两个4长度的有序子序列，如何归并成一个，其实类似于多项式加法中我们所弄出来的3个指针。
                    显然时间复杂度就是大on，但是我们首先要得到这个有序的子序列
                    void merge(EleType A[],Eletype tmpA[],int L,int R,int RightEnd){
                        leftEnd=R-1
                        tmp=L
                        NumElement=RightEnd-L+1
                        while(L<=leftEnd&&R<=RightEnd){//当左序列或者右序列没到顶的时候
                            if(A[L]<= A[R])tmpA[tmp++]=A[L++]
                            else  tmpA[tmp++]=A[R++]
                        }

                      while(L<=leftEnd)tmpA[tmp++]=A[L++]
                     while(R<=RightEnd)tmpA[tmp++]=A[R++]
                        //这里把两个后面的尾巴给补足 也是只会执行一次，归并的使用场景就是两个有序序列

                    for i-- elementNum  把元素倒着插会A[]
                    }
                    归并在实现的时候其实有两种不太一样的策略，第一种是递归分而治之
                    分治算法的 Tn=NlogN 没有最好 最坏 或者平均的情况 就是NlogN 可以说是比较强的NlogN
                    而且这个分治实现递归的确是明显，但是自己脑内执行总感觉怎么这么麻烦呢
                    void Msort(A[],tempA[],L,RightEnd){
                        int center
                        if(L< RightEnd){
                            Center=(L+RightEnd)/2
                            Msort(A,tempA,L,Center)
                            Msort(A,tempA,center+1,RightEnd)
                            Merge(A,tempA,L,center+1,RightEnd)
                    }
                    }
                    //两个递归来整理前面的子序列   递归其实也在无限的Merge 能想出这个办法的人真的不一般
                    Tn=2T(N/2)+ON  这样无限的去弄 N=2^k 差不多这样就能推出NlogN 对付一些递归的方法 归并比较好的地方就是稳定
                    所以还是要统一我们的数据结构 对于上面的形式参数去做一个提前init，不要重复的malloc临时数组，一开始就建立好直接传进去是一个比较好的做法，
                    即使临时变量似乎并不关键。
                    当然递归并不是那么美妙要占用系统的堆栈空间，我们来讲讲非递归算法，相邻两个字序列归并，不停这么做。归并排序非递归的空间复杂度是ON，而且需要额外开一个空间来存储临时的组。
                    我们想象一个麻将堆成的倒金字塔，他就可以类似的推成我们的一个有序序列。代码一开始的不完整给了我一些疑惑。最开始代码如下

                    void merge_pass(A tempA N length){//新开数组tempA 为外围的有数组情况下，不要按照倒麻将内存去搞那么多中间内存。
                        for(i=0 i< N-2*length;i+=length*2){ //i从零开始每次跳两个length长度去
                        merge_1(A,tempA,i,i+length,i+length*2-1) //因为这个是吧A 结果merge到tempA 上了 这后面三个量应该是左 中 右 三个量了 用来做归并的
                    //至于在 N-2length 地方停止无非是 N<4length  这个地方的划分语言层面的解释可能有苍白所以得举例子，就类似于 i=0 i< length -1 i++ 一样从数学的角度
                    我们把+1 变成+狗 不就有 i< N-2*length i+=length*2,至于这里为什么是 -2*length 我们就需要多预留2*length 长度的意思。
                    多嘴一句 我们每次要for 循环 每次跳两个length 长度 两个length的序列来归并
                    //此外 在归并 并不总是完美的只剩下一个，而是有可能是剩下两个序列的情况 这个时候我们就要去检查最后的 N-length 还有多少空余长度，把最后那段长度最后
                    做一次归并
                    }
                    if(N> i+length)merge_1(A,tempA,i)merge_1(A,tempA,i,i+length,N-1)// 这一步的用意到底是在干嘛 真心的搞不懂  哦 懂了 也就是 当长度 在 length 到 2length 之间的余段 要把后面的两小段他们俩给归并好 当然他也只是执行一次 所以不在乎是在for循环内外了
                    else 刚好就剩一个组 就把 i-n 最后的收进来
                    他妈的眼睛又看花了 我还以为是在for循环里面呢  结果果然如同我所想 就是在外面才对嘛 所以下面那个执行结果一定是在A里面 tempA里不是
                    }

                    最后我们的sort算法 接口要统一，所以最后的merge_sort

                    void merge_sort(elementtype A[] ,int N){
                        malloc tempA
                        while(length< N){
                        mergePass(A,tempA,N,length)
                        length*2 这里超越了 也会导回
                        mergePass(tempA,A,N,length)
                        length*2
                    //反向归置 tempA[]回到A[]  这样最后一定是会被归置会A 上并且 这种左右互导最终一定会回到A
                    //情况去推导  假设在  m1做完 length*2 length超标  m2 并不会执行 这样还是在tempA里
                    }
                    }

                    稳定算法 上界下界 平均 稳定 这里好 那里好 千好万好都是
                </span>
            </p>

            <p>10.排序 下
                <span>
                    1.快速排序  传说中在现实当中应用最广泛的算法，但是在现实的应用场景下不同算法都有其优缺点，不一定是谁最快。一般而言快排都是最快。大规模的随机数据快排很快，但是快排你的设计很多细节，
                    这也是算法时空说 STL库中快排以及各种算法的国家队的原因，自己写快排序非常容易出错，而且不容易实现最快。先来看看算法概述，跟归并算法的设计思想有点类似，都是使用了分而治之的思想，
                    pivot 主元。其算法分治就是先从序列找到一个元素去做主元，把小于放左边大于放右边，然后继续递归的去做分治，最后得到的序列放回到数组当中去。
                    我们先来看一下伪代码
                    void quickSort(a[],N){
                    if N<2 return
                    pivot=s中的一个
                    s={
                    quickSort(a[],N1)prvot 之前的s  这力0->n-1 其实算半个On 这种意思你懂吧
                    pivot
                     quickSort(a[],N2)之后的s
                    }
                    }
                    我们要设计很多的细节包括 主元怎么着 找的不好快排很慢 ，如何快速的分出主元的前后集合，最后在导回A数组当中去
                    选 主元 ---- 实现的不好就是递归的On2
                        取随机数
                        取中位数median
                        meidian3 这里有一些小技巧
                    void median3(){//查找三个数当中的median  当然这里的left right 都是index 并不是真正的指针或者其他数
                        int center=(left+right)/2
                        if(A[left]>A[center])swap(A[left],A[center])
                        if(A[left]>A[right])swap(A[left],A[right])
                        if(A[center]>A[right])swap(A[center],A[right])

                    //这里要做一个操作
                    swap(A[center]>A[right-1]) 把主元放在rigth -1 也就是子列最右边
                    最后我们只需要考虑 left+1 ->right -2 这个列
                    return A[right -1]
                    }
                    快速排序为什么快 就是因为主元被一次性的放到正确的位置
                    当元素与主元相同的时候，依旧要交换虽然在相等比较多的情况比较傻，但是好处就是主元会停留在中间的位置。
                    但是假如我们去避免这个没有必要的交换，主元依旧停留边缘处，而且
                    快速排序是使用递归的，递归是非常的不好的，使用的ON内存，所以快排对于小规模 N不到100个情况反而不如插入排序
                    但是对于大规模的递归的作用就体现出来了，但是希望中间的内存不去多小号，我们的数组希望是全局变量
                    这样我们就可以定义一个 cut off的阈值，去比较效率的影响  比如我们快排到cutoff 为100长度 就是 left+1 - right+2 <100 的情况我们就用插入排序

                    我们的算法实现就是
                    void quick_sort(Element A ,int left ,int right){
                    if cut_off< right -left :
                        pivot=medium3(left,right,A)
                        i=left j=left -1
                        for(;;){
                        while(A[++i]< pivot)//当不小于是跳出
                        while(A[--j]> pivot)//不大于时跳出  两个红色警报
                        if(i-j<0)swap(A[i],A[j])
                        else break
                    }
                    swap(A[i],A[right-1])//主元跟i交换
                    quick_sort(A,left,i-1);//左边
                    quick_sort(A,i+1,right)//右边

                    else :
                        insert_sort(A+left，right-left +1)
                    }
                    这个quick_sort的接口不对 要 quick_sort(A,length)  left=0 rigth =length -1 在里面弄一个初始化


                    不要仅仅只作老师 还要做开发，心性的丧失水平也会随之下降。 用了去中心化的交易所怎么感觉这么难用呢，真是的。
                    dot uni  1inch  感觉要用新的估值模型

                    2.表排序
                    接下来我们介绍表排序，使用场景并不仅仅简单是数字的比较，而是比较一些结构，就是字典序很多order by 等等数据库当中的排序，不移动元素而是移动指针
                    A       0   1   2   3   4   5   6   7
                    key     f   d   c   a   g   b   h   e
                    table   0   1   2   3   4   5   6   7
                最终 table  3   5   2   1   7   0   4   6

                    最后我们输出table不需要去移动只需要除数 A[table[0]]  A[table[1]]    A[table[2]] ... A[table[n]]  就完事了 我他们我以前就想到这种映射排序法。
                    一个定理 就是N个数字的排列就是若干个独立的环组成。没有交集的环，这意味着ON，对于独立环还意味他们能够被转动，就是假设一定要有物理排序的话，
                    将table[index]=index 就代表单独的环 而不需要去多做改变了。
                    算法的时间复杂度
                    最坏情况就是 N/2个环，没两个环之间都有3个交换 就是O MN
                    3.基数排序
                    有定理说明，就是前面所有数字的交换比较，即使跑得再快不管再怎么弄 都有一个最坏的下界 NlogN，我们的基数排序就是干一些其他的事情。
                    先来介绍桶排序。其使用场景如下
                    我们的浙大有4w个学生 成绩就是0-100之间 有101个数值。就是hashtable   OM+N
                    我们建立这个101个桶链表，给这4w个学生插入到桶链表当中。但是当我们的 N只有10 而M为1000的时候突然发现桶排序就不合算了。
                    考虑基数排序，也就是LSD 数据的最末位10进制，把他们放入到这10进制个数的10个桶链表当中。782 放到2这个桶里 284放到4这个桶，
                    第一趟是这样的只考虑个位数，第二趟考虑10位数 扫描第一个桶，然后插入新10个桶就是第二个桶 ，第三个就是扫第二个桶再建立10个桶，这样
                    O P(N+B) P = logM  B就是进制
                    有其他用处，多关键字的排序 扑克牌的花色与数字排序  LSD一般情况都比MSD快 主排序

                    4.几个排序算法的比较
                    排序方法            平均时间复杂度             最坏情况时间复杂度           额外空间复杂度             稳定性
                    简单选择            N2                          N2                          1                           不稳定  跳着交换
                    冒泡排序            N2                          N2                          1                           稳定
                    直接插入            N2                          N2                          1                           稳定
                    希尔排序            N^d                         N2                          1                           不稳定  看增量序列选的怎么样 D<2
                    堆排序              NlogN                      NlogN                        1                           不稳定  表面非常完美 但是NlogN还有一个常数相乘
                    快速排序            NlogN                       N2                          LogN                        不稳定  时间复杂度最好情况下 也需要OLogN 额外空间
                    归并排序            NlogN                       NlogN                       ON                           不稳定
                    基数排序            P(N+B)                      P(N+B)                      ON+B                        稳定 会莫衷情况下打破NlogN的魔咒 线性取决于几个桶

                    这个总结下来 我都尿了，这他妈的我有点不熟悉这个最坏情况 而且 直接插入怎么就是On2 不熟悉 额外空间复杂度 感觉应该还有其他

                    假精致害人啊


                    马云被查 现代社会很虚弱   汇顶科技
                    现在的这帮技术宅二次元初中高中生，二次元头像是真的好厉害。我这种业务开发程序员简直都不叫程序员了。
                    失败的一天


                    微博 最初的动力就是看别人去推荐书，这次又新看见一个V，总之跟之前的知乎V 阿莱克 一样
                    推荐  homl  ddia fosa 等等 这些程序员真需要的书 看他微博什么三阶马尔可夫什么好像很吊的样子  数学也能做
                    The Probabilistic Method, Third Edition 推荐书真的 可太可了

                    难受住 今天 1.4 难受住 btc 3.2w刀了 eth 昨天900刀 今天 1100刀  40% 可太难受了 逻辑就是btc大部分已经进入机构的怀里 接下来就是以太坊了。
                    平台币可没有主流币快啊 难受住   uni 所有资产的价格都在暴涨  不配置资产是不行的
                </span>
            </p>
            <span>说起算法 我就很喜欢素数算法以及那些非常有用的算法。就像是各种工业界的算法，离散傅立叶变换算法这些等等 等等  复变 实变 泛函</span>

            <p>11.散列查找
                <span>
                    哇靠回来了，何老师。c语言的两个场景 变量是临时变量与全局变量。
                    插入 查找 删除 动态方法  查找树 AvL树
                    验证两个字符串相等 一个个比太慢可 换个散列hash的思路，就舒服了。
                    查找的本质就是给定对象去找它所存储的位置，
                    两种方法 一种是有序的排列，另一种是  就是 hashmap的意思 O(1)的时间复杂度并且去解决我们的冲突问题，用链表链接在后面，取余来
                    装填因子 n/m  collision 碰撞
                    直接定址  直接取余  数字分析法(随机取位置  字符的话 -'0' *10^4)  对啊 法律
                    位折叠法  平方取中法

                    构造函数   比如我们的 h(abcde)=a*32^4 + b*32^3 + c*32^2 + d*32^1 e*32^0  求级数有前面的一种比较简单的方法。
                    冲突的解决方法 线性探测 d=i 1 2 3 4 5  平方探测 土 i2   1 -1 4 -4 9 -9 假如超出了就求余  求回来。
                    双散列

                    线性探测的问题就是容易发生聚集  散列表还有一个值就是 ASLs 就是平均成功查找次数
                    ASLu  平均查找不成功次数

                    平方探测避免了线性探测的聚集性问题，但是会出现有些空位永远插不进去的问题。有定理显示 当我们的空间 tableSize为 4K+3 k为素数的时候
                    我们一定能够用平方探测法找到全空间，可以开个结构数组 多一个cNum的碰撞次数的字段，来区分碰撞次数以及我们的。而且删除的时候要小心，要添加一个delete标记防止短链。
                    双散列的话 h(key2)=p-(key mod p)    p< tableSize p tableSize 都是素数

                    再散列  就是散列表被装的比较满了 效率比较低了，散列表扩大，原先的散列树都得重新算  0.5< a < 0.85  其实>0.5 就已经是部分不可接受的了，考虑去重新散列，
                    分离链表法 就是hashmap原理 做到链表上

                    散列表的性能跟 散列函数是否均匀 a的大小 处理冲突的方法 三者有关
                    对于线性探测法 是如下
                    p= 期望探测次数
                    1.不成功 1/2*[1+  ( 1/(1-a)2)]
                    2.成功1/2*[1+  ( 1/(1-a))]
                    线性探测 我们所知道的有一个跟a相干的函数

                    对于平方探测法是如下
                       p= 期望探测次数
                    1.不成功  1/(1-a)
                    2.成功-1/a*ln(1-a)


                    3 分离链表法 a 有可能是大于1的
                    p= 期望探测次数
                    1.不成功  a+e^-a
                    2.成功 1+a/2

                    不同碰撞解决办法的散列 ASLs ASLu 随着a的变化而变化的曲线

                    几乎跟空间大小N无关 也跟字符串的计算量大小无关  比较好的查找列
                    是以较小的a值为前提并且不适合我们的顺序序列 不适合查找最大最小值  本质上就是以空件换时间
                    分离链表法 ，链表的查找跟插入 存储的效率都比较低但是链表的删除比较方法不需要留下delete标记，从而并没有存储垃圾，同样的查找因子比较大的
                    时候效率会下降

                    文本词频率  -分词  hash算出词频  等等两个办法  这个办法好啊。  好了 学完了 但是似乎好像并没有 kmp啊这样的算法之类的


                    AI GPT OPENAI  alpha  等等都给了我极大的震惊，所以2021 我要学会AI
                </span>
            </p>

            <p>12.综合习题  PAT 小白专场我都没兴趣去看
                <span>

                </span>
            </p>

        </div>
    </body>

</html>
